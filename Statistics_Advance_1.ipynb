{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the properties of the F-distribution.\n",
        "\n",
        "The F-distribution is a continuous probability distribution that arises frequently in statistical analysis, particularly in the context of analysis of variance (ANOVA) and hypothesis testing. Here are the key properties and characteristics of the F-distribution:\n",
        "\n",
        "1. Definition and Application: The F-distribution is used to compare two variances and test hypotheses about the ratio of variances. It is often used in ANOVA to determine if there are significant differences between group means.\n",
        "\n",
        "2. Shape of the Distribution: The shape of the F-distribution is asymmetric and skewed to the right. The skewness decreases as the degrees of freedom increase, making the distribution approach normality. It is non-negative, meaning all values are greater than or equal to zero, as variance cannot be negative.\n",
        "\n",
        "3. Degrees of Freedom: The F-distribution is defined by two sets of degrees of freedom: numerator degrees of freedom (d1) and denominator degrees of freedom (d2). The degrees of freedom determine the exact shape of the distribution.\n",
        "\n",
        "4. Mean and Variance: The mean of the F-distribution is given by d2 / (d2 - 2) for d2 > 2. The variance is more complex and is defined by: Variance = 2 * (d1 + d2 - 2) / (d1 * (d2 - 2)^2 * (d2 - 4))\n",
        "\n",
        "5. Relation to Other Distributions: The F-distribution is a ratio of scaled chi-squared distributions. Specifically, if X1 ~ χ2(d1) and X2 ~ χ2(d2), then: F = (X1 / d1) / (X2 / d2) follows an F-distribution with degrees of freedom d1 and d2.\n",
        "\n",
        "6. Right-Tailed Test: The F-distribution is primarily used in right-tailed tests because it measures the ratio of variances. Most of the critical values for hypothesis tests are found on the right side of the distribution.\n",
        "\n",
        "7. Non-Symmetry: Unlike the normal or t-distributions, the F-distribution is not symmetric. Its shape is influenced by the degrees of freedom, becoming less skewed with higher degrees of freedom.\n",
        "\n",
        "8. Key Use Cases: ANOVA (Analysis of Variance), Regression Analysis, and testing equality of two variances.\n",
        "\n",
        "9. Critical Values: Critical values for the F-distribution can be found in F-distribution tables or calculated using statistical software. These values are used to determine the outcome of hypothesis tests based on the ratio of variances.\n",
        "\n",
        "10. Non-Negative Values: The F-distribution only takes positive values, as it represents the ratio of variances, which are inherently non-negative.\n"
      ],
      "metadata": {
        "id": "Oy3EvUZQaWNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "\n",
        "The F-distribution is primarily used in statistical tests that involve comparing variances or assessing the overall fit of a model. It is appropriate for these types of tests due to its properties as a ratio of two independent variances, making it ideal for evaluating relative differences between sample variances or the explanatory power of models. Below are the main types of statistical tests where the F-distribution is used:\n",
        "\n",
        "1. Analysis of Variance (ANOVA)\n",
        "Purpose: ANOVA is used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others.\n",
        "Why F-distribution is appropriate: ANOVA involves partitioning total variation in the data into variation between groups and within groups. The F-statistic is calculated as the ratio of the variance between group means (mean square between) to the variance within the groups (mean square within). The F-distribution provides the critical value for this ratio, helping to determine whether the observed group differences are statistically significant.\n",
        "2. Regression Analysis\n",
        "Purpose: The F-test in regression analysis assesses whether a model provides a better fit to the data compared to a model with no predictors.\n",
        "Why F-distribution is appropriate: In regression, the F-test evaluates the overall significance of the model by comparing the explained variance (due to the regression) with the unexplained variance (residuals). The F-statistic is the ratio of the mean square due to the regression (explained variance) to the mean square due to residuals (unexplained variance). This ratio follows an F-distribution under the null hypothesis that the model has no explanatory power.\n",
        "3. Test for Equality of Two Variances\n",
        "Purpose: This test is used to compare the variances of two independent samples to check if they come from populations with equal variances.\n",
        "Why F-distribution is appropriate: The F-statistic in this test is the ratio of the variances of the two samples. The F-distribution is used because it accurately models the distribution of this ratio under the null hypothesis that the two population variances are equal.\n",
        "4. Comparison of Nested Models\n",
        "Purpose: The F-test is often used to compare a more complex model (with more parameters) against a simpler nested model (with fewer parameters) to determine if the added complexity is justified.\n",
        "Why F-distribution is appropriate: The F-statistic measures the ratio of the increase in explained variance (due to the additional parameters) to the unexplained variance. This ratio follows an F-distribution, which allows for assessing whether the additional parameters significantly improve the model's fit.\n",
        "5. Two-Way ANOVA\n",
        "Purpose: Used to evaluate the effect of two independent factors and their interaction on a dependent variable.\n",
        "Why F-distribution is appropriate: The F-distribution helps compare the variance due to each factor and their interaction to the within-group variance, determining if these effects are significant.\n",
        "6. MANOVA (Multivariate Analysis of Variance)\n",
        "Purpose: Extends ANOVA to handle multiple dependent variables simultaneously.\n",
        "Why F-distribution is appropriate: Although MANOVA uses a more complex distribution, the F-test is still part of its analysis to evaluate the significance of variances among the linear combinations of dependent variables.\n",
        "Why is the F-distribution Appropriate?\n",
        "Ratio of Variances: The F-distribution is ideal for tests involving variances because it models the ratio of two chi-squared distributions (scaled by their respective degrees of freedom).\n",
        "Right-Tailed Test: Most of these tests (ANOVA, regression significance) use a right-tailed F-test since they assess whether the observed variance ratio is larger than expected under the null hypothesis.\n",
        "Degrees of Freedom Dependence: The F-distribution adjusts based on the degrees of freedom associated with the numerator and denominator, providing flexibility for various comparisons."
      ],
      "metadata": {
        "id": "dsJ1z0TfayOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "populations?\n",
        "\n",
        "Conducting an F-test to compare the variances of two populations involves certain key assumptions to ensure the validity of the results. Here are the main assumptions required for an F-test:\n",
        "\n",
        "1. Independence of Samples\n",
        "Assumption: The two samples being compared must be independent of each other. This means that the selection or outcome of one sample should not influence or affect the selection or outcome of the other.\n",
        "Reason: Independence ensures that the test's outcome reflects the true variability between the populations without any influence from sample dependence.\n",
        "2. Normality of the Populations\n",
        "Assumption: Both populations should follow a normal distribution.\n",
        "Reason: The F-test is sensitive to deviations from normality, especially if the sample sizes are small. If the populations are not normally distributed, the test might lead to inaccurate conclusions. For larger sample sizes, the Central Limit Theorem may reduce the impact of this assumption, making the test more robust to deviations from normality.\n",
        "3. Random Sampling\n",
        "Assumption: The data samples should be randomly selected from the populations of interest.\n",
        "Reason: Random sampling ensures that the samples are representative of the populations, reducing the risk of sampling bias and making the test results more generalizable.\n",
        "4. Positive Variances\n",
        "Assumption: The variances of the populations should be positive (greater than zero).\n",
        "Reason: Since the F-statistic is a ratio of variances, it requires that both the numerator and the denominator represent positive values. A variance of zero or negative is not meaningful in this context.\n",
        "5. Homoscedasticity (for ANOVA)\n",
        "Note: While not directly an assumption of the F-test for comparing two variances, the F-test used in ANOVA assumes homoscedasticity, meaning the variances within each group being compared should be roughly equal.\n",
        "Reason: Homoscedasticity ensures that the F-statistic accurately reflects the differences between group means without being influenced by unequal variances."
      ],
      "metadata": {
        "id": "lOdqUC2QbIlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "\n",
        "Purpose of ANOVA\n",
        "ANOVA, or Analysis of Variance, is a statistical method used to compare the means of three or more groups to determine if there is a statistically significant difference between them. The purpose of ANOVA is to test the null hypothesis that all group means are equal against the alternative hypothesis that at least one group mean is different.\n",
        "\n",
        "Key Differences Between ANOVA and a t-test\n",
        "While both ANOVA and a t-test are used to compare means, they are suited for different scenarios. Here are the main differences:\n",
        "\n",
        "1. Number of Groups Compared\n",
        "t-test: Compares the means of two groups.\n",
        "ANOVA: Compares the means of three or more groups.\n",
        "2. Risk of Type I Error\n",
        "t-test: When comparing more than two groups with multiple t-tests, the risk of committing a Type I error (false positive) increases. This means the probability of incorrectly rejecting the null hypothesis rises with each additional comparison.\n",
        "ANOVA: Controls the Type I error rate by comparing all groups simultaneously in a single test, maintaining a consistent overall error rate. This makes it more appropriate for comparing multiple groups.\n",
        "3. Hypothesis Tested\n",
        "t-test: Tests the null hypothesis that the means of two groups are equal.\n",
        "ANOVA: Tests the null hypothesis that all group means are equal. It does not specify which groups are different; it only indicates whether at least one group differs from the others. Post-hoc tests (e.g., Tukey's test) are needed to determine which specific groups are significantly different.\n",
        "4. Application\n",
        "t-test: Used for comparing the means of two independent samples (independent samples t-test) or the means of two related samples (paired samples t-test).\n",
        "ANOVA: Used when there are three or more groups or factors being compared. It can also be extended to two-way ANOVA or multi-way ANOVA to study the interaction between different factors.\n",
        "5. Calculation and Output\n",
        "t-test: Produces a t-statistic and a corresponding p-value that helps determine if the difference between two groups is statistically significant.\n",
        "ANOVA: Produces an F-statistic, which is the ratio of the variance between group means to the variance within the groups. The corresponding p-value indicates whether the differences between group means are statistically significant.\n",
        "When to Use ANOVA vs. a t-test\n",
        "t-test: Use when comparing the means of only two groups.\n",
        "ANOVA: Use when comparing the means of three or more groups or when analyzing the interaction effects between two or more independent variables.\n",
        "Example Scenario\n",
        "t-test: Comparing the average exam scores of students in two different classes to see if one class performed better than the other.\n",
        "ANOVA: Comparing the average exam scores of students across three different classes to see if there are any significant differences among the classes."
      ],
      "metadata": {
        "id": "v7mCcD0gbZgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "than two groups.\n",
        "\n",
        "The F-distribution is a continuous probability distribution that is widely used in statistical tests to compare variances and test hypotheses about the ratio of variances. It is commonly used in the following types of statistical tests:\n",
        "\n",
        "1. Analysis of Variance (ANOVA): The F-distribution is used to test the ratio of variances between two or more groups to determine if there are significant differences between means. In ANOVA, the F-statistic is calculated as the ratio of the variance between groups to the variance within groups. The F-distribution is used to determine the critical values and p-values for this test.\n",
        "\n",
        "2. Regression Analysis: The F-distribution is used in the F-test to assess the overall significance of a regression model. The F-test evaluates the ratio of the variance explained by the regression model to the variance not explained by the model. The F-distribution is used to determine the critical values and p-values for this test.\n",
        "\n",
        "3. Equality of Two Variances: The F-distribution is used to test the null hypothesis that two populations have equal variances. This test is commonly used to determine if two groups have similar variability.\n",
        "\n",
        "The F-distribution is appropriate for these tests because:\n",
        "\n",
        "- Accounts for the ratio of variances: The F-distribution is designed to work with the ratio of variances, which is essential in ANOVA and equality of variances tests.\n",
        "- Considers the degrees of freedom: The F-distribution takes into account the degrees of freedom, which is critical in regression analysis and other tests.\n",
        "- Robust and accurate: The F-distribution provides a robust and accurate way to test hypotheses about variances and means.\n",
        "- Allows for calculation of critical values and p-values: The F-distribution enables researchers to calculate critical values and p-values, which are essential for making informed decisions about their data.\n",
        "\n",
        "Additionally, the F-distribution has several important properties, including:\n",
        "\n",
        "- Non-negative values: The F-distribution only takes positive values, as it represents the ratio of variances.\n",
        "- Asymmetric shape: The F-distribution is skewed to the right, with a long tail.\n",
        "- Degrees of freedom: The F-distribution is defined by two sets of degrees of freedom: numerator degrees of freedom (d1) and denominator degrees of freedom (d2).\n",
        "\n",
        "Overall, the F-distribution is a powerful tool in statistical analysis, and its applications continue to expand into various fields, including medicine, social sciences, and engineering. Its ability to account for the ratio of variances and consider the degrees of freedom makes it an essential distribution in statistical testing."
      ],
      "metadata": {
        "id": "D3t0DuP1bjMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?\n",
        "\n",
        "The partitioning of variance in ANOVA is a crucial step in understanding the relationships between the variables. It helps to isolate the effect of the independent variable on the dependent variable by breaking down the total variability in the data into different components.\n",
        "\n",
        "Total Variance (SST)\n",
        "\n",
        "The total variance (SST) is the overall variability in the data from the grand mean. It is calculated as:\n",
        "\n",
        "SST = ∑(xi - x̄)**2\n",
        "\n",
        "where xi is each observation, x̄ is the grand mean, and n is the total number of observations.\n",
        "\n",
        "Between-Group Variance (SSB)\n",
        "\n",
        "The between-group variance (SSB) measures how much the group means differ from the grand mean. It is calculated as:\n",
        "\n",
        "SSB = ∑ni(x̄i - x̄)**2\n",
        "\n",
        "where ni is the number of observations in each group, x̄i is each group mean, and x̄ is the grand mean.\n",
        "\n",
        "Within-Group Variance (SSW)\n",
        "\n",
        "The within-group variance (SSW) measures the variability within each group, reflecting how data points differ from their respective group means. It is calculated as:\n",
        "\n",
        "SSW = ∑(xi - x̄i)**2\n",
        "\n",
        "where xi is each observation, x̄i is each group mean, and ni is the number of observations in each group.\n",
        "\n",
        "F-Statistic\n",
        "\n",
        "The F-statistic is calculated by dividing the mean square between groups (MSB) by the mean square within groups (MSW):\n",
        "\n",
        "F = MSB / MSW\n",
        "\n",
        "MSB = SSB / (k - 1)\n",
        "\n",
        "MSW = SSW / (n - k)\n",
        "\n",
        "where k is the number of groups, and n is the total number of observations.\n",
        "\n",
        "Interpretation\n",
        "\n",
        "The F-statistic is a ratio of the variance between groups to the variance within groups. A significantly larger F-statistic indicates that the between-group variance is greater than the within-group variance, suggesting that at least one group mean is different from the others.\n"
      ],
      "metadata": {
        "id": "JUCRIrynb3N4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n",
        "\n",
        "The classical (frequentist) approach to ANOVA and the Bayesian approach to ANOVA are both used for analyzing differences between group means, but they differ fundamentally in their treatment of uncertainty, parameter estimation, and hypothesis testing.\n",
        "\n",
        "Handling of Uncertainty\n",
        "\n",
        "Frequentist ANOVA:\n",
        "\n",
        "- Objective probability: Treats probabilities as long-term frequencies of events.\n",
        "- Fixed parameters: Assumes parameters such as group means and variances are fixed but unknown values.\n",
        "- Confidence intervals: Provides confidence intervals for parameter estimates, which indicate a range of plausible values that would cover the true parameter in a certain percentage of repeated samples.\n",
        "\n",
        "Bayesian ANOVA:\n",
        "\n",
        "- Subjective probability: Treats probabilities as a measure of belief or confidence in an event based on prior knowledge and data.\n",
        "- Parameters as random variables: Assumes that parameters have probability distributions, incorporating uncertainty directly through prior and posterior distributions.\n",
        "- Credible intervals: Provides credible intervals, which indicate the probability that the true parameter lies within the specified range, given the data and prior information.\n",
        "\n",
        "Parameter Estimation\n",
        "\n",
        "Frequentist ANOVA:\n",
        "\n",
        "- Point estimates: Uses sample data to provide point estimates (e.g., sample means) of the parameters.\n",
        "- Maximization: Relies on maximizing the likelihood of observing the sample data to estimate parameters.\n",
        "- Single estimate: Each parameter has a single best estimate, with its uncertainty captured by the standard error.\n",
        "\n",
        "Bayesian ANOVA:\n",
        "\n",
        "- Prior information: Starts with a prior distribution that reflects initial beliefs about the parameters before observing the data.\n",
        "- Posterior distribution: Updates the prior with observed data using Bayes' theorem to generate a posterior distribution, which reflects the updated beliefs about the parameters after considering the data.\n",
        "- Full distribution: The estimation results in a distribution for each parameter, not just a single point estimate.\n",
        "\n",
        "Hypothesis Testing\n",
        "\n",
        "Frequentist ANOVA:\n",
        "\n",
        "- Null hypothesis (H0): Tests whether all group means are equal (e.g., H0: μ1 = μ2 = ... = μk).\n",
        "- p-value: Provides a p-value to determine if there is enough evidence to reject the null hypothesis.\n",
        "- Type I/II errors: Involves considerations of Type I and Type II error rates (e.g., false positives and false negatives).\n",
        "\n",
        "Bayesian ANOVA:\n",
        "\n",
        "- Posterior probability: Directly calculates the probability of hypotheses.\n",
        "- Bayes factor: Often uses Bayes factors as an alternative to the p-value to measure the strength of evidence for one hypothesis over another.\n",
        "- Incorporation of prior beliefs: The approach can integrate prior beliefs about group differences into the analysis, impacting the posterior probabilities.\n",
        "\n",
        "Interpretation of Results\n",
        "\n",
        "Frequentist ANOVA:\n",
        "\n",
        "- p-value interpretation: The result indicates whether the observed data is likely under the null hypothesis.\n",
        "- Fixed conclusion: A significant result (p-value < 0.05) suggests rejecting the null hypothesis, while a non-significant result suggests failing to reject it.\n",
        "\n",
        "Bayesian ANOVA:\n",
        "\n",
        "- Probability interpretation: The outcome directly reflects the probability of the hypothesis given the data and the priors.\n",
        "- Flexible decision-making: Offers a more nuanced conclusion based on the posterior distributions, allowing for more flexible decision-making than binary outcomes from frequentist methods.\n",
        "\n",
        "Key Differences\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "- Treats parameters as fixed\n",
        "- Uses p-values and point estimates\n",
        "- Tests hypotheses without prior beliefs\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "- Treats parameters as random variables\n",
        "- Incorporates prior information\n",
        "- Uses posterior distributions and credible intervals\n",
        "- Provides direct probabilities for hypotheses\n"
      ],
      "metadata": {
        "id": "MlIKl9CacW7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "8. Question: You have two sets of data representing the incomes of two different professions1\n",
        "V Profession A: [48, 52, 55, 60, 62'\n",
        "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison\n",
        "\n",
        "\n",
        "Data:\n",
        "\n",
        "Profession A: 48, 52, 55, 60, 62\n",
        "\n",
        "Profession B: 45, 50, 55, 52, 47\n",
        "\n",
        "Hypotheses for the F-test:\n",
        "\n",
        "Null Hypothesis (H0): The variances of the two professions' incomes are equal (σ_A^2 = σ_B^2)\n",
        "\n",
        "Alternative Hypothesis (H1): The variances of the two professions' incomes are not equal (σ_A^2 ≠ σ_B^2)\n",
        "\n",
        "F-statistic Formula: F = s_A^2 / s_B^2\n",
        "\n",
        "Where:\n",
        "\n",
        "- s_A^2 and s_B^2 are the sample variances of Profession A and Profession B, respectively.\n",
        "\n",
        "Python Code for Calculation:\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Given data\n",
        "profession_A = [48, 52, 55, 60, 62]\n",
        "profession_B = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate sample variances\n",
        "var_A = np.var(profession_A, ddof=1)\n",
        "var_B = np.var(profession_B, ddof=1)\n",
        "\n",
        "# Calculate F-statistic\n",
        "F_statistic = var_A / var_B\n",
        "\n",
        "# Degrees of freedom\n",
        "dof_A = len(profession_A) - 1\n",
        "dof_B = len(profession_B) - 1\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = 2 * min(f.cdf(F_statistic, dof_A, dof_B), 1 - f.cdf(F_statistic, dof_A, dof_B))\n",
        "\n",
        "# Output results\n",
        "print(f\"F-statistic: {F_statistic:.2f}\")\n",
        "print(f\"P-value: {p_value:.2f}\")\n",
        "\n",
        "\n",
        "#Output:\n",
        "#F-statistic: Approximately 2.09\n",
        "#P-value: Approximately 0.75\n",
        "#Interpretation:\n",
        "#F-statistic: The ratio of the variance of Profession A to Profession B is approximately 2.09.\n",
        "#P-value: The p-value of approximately 0.75 is much greater than typical significance levels (e.g., 0.05 or 0.01).\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8VHlcv8dt25",
        "outputId": "d47bf95f-2db7-4382-8d64-7a323834d46b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.09\n",
            "P-value: 0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data1\n",
        " Region A: [160, 162, 165, 158, 164']\n",
        " Region B: [172, 175, 170, 168, 174']\n",
        " Region C: [180, 182, 179, 185, 183']\n",
        " Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        " Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
        "\n",
        "\n",
        "To conduct a one-way ANOVA to test for statistically significant differences in average heights between the three regions, we'll use Python's scipy.stats library. Here is the code along with the interpretation of the results:\n",
        "\n",
        "Data Provided:\n",
        "Region A: [160, 162, 165, 158, 164]\n",
        "Region B: [172, 175, 170, 168, 174]\n",
        "Region C: [180, 182, 179, 185, 183]\n",
        "Python Code:\n",
        "'''\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Heights data for each region\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "# Print results\n",
        "print(f\"F-statistic: {f_statistic:.2f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "\n",
        "#Interpretation\n",
        "\n",
        "# F-statistic: This value indicates the ratio of the between-group variance to the within-group variance.\n",
        "# P-value: This value helps us determine whether to reject or fail to reject the null hypothesis.\n",
        "\n",
        "#Hypotheses\n",
        "\n",
        "# Null Hypothesis (H0): The average heights between the three regions are the same.\n",
        "# Alternative Hypothesis (H1): At least one region has a different average height.\n",
        "\n",
        "#Let me know if you'd like me to help with anything else!\n",
        "\n",
        "#Formatted Output\n",
        "\n",
        "#Interpretation\n",
        "\n",
        "# F-statistic: Ratio of between-group variance to within-group variance\n",
        "# P-value: Determines whether to reject or fail to reject the null hypothesis\n",
        "\n",
        "#Hypotheses\n",
        "\n",
        "# Null Hypothesis (H0): Average heights are the same across three regions\n",
        "# Alternative Hypothesis (H1): At least one region has a different average height"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "875qj3fk_6Pd",
        "outputId": "9e2879d2-2fb4-470b-fbd9-1ca2d49eb21c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87\n",
            "P-value: 0.0000\n"
          ]
        }
      ]
    }
  ]
}